---
title: "Week 6: Multivariate Distributions"
subtitle: "*DSAN 5100: Probabilistic Modeling and Statistical Computing*<br>Section 01"
author: "Purna Gamage, Amineh Zadbood, and Jeff Jacobs"
date: 2025-09-30
date-format: full
lecnum: 8
bibliography: "../_DSAN5100.bib"
cache: false
categories:
  - "Class Sessions"
format:
  revealjs:
    footer: "DSAN 5100 W06: Multivariate Distributions"
    output-file: slides.html
    df-print: kable
    code-fold: true
    html-math-method: mathjax
    scrollable: true
    slide-number: true
    number-sections: false
    tbl-cap-location: bottom
    link-external-icon: true
    link-external-newwindow: true
    theme: [default, "../dsan-globals/jjquarto.scss"]
    revealjs-plugins:
      - simplemenu
    simplemenu:
      flat: true
      barhtml:
        header: "<div class='menubar'><span style='position: absolute; left: 8; padding-left: 8px;'><a href='./index.html'>&larr; Return to Notes</a></span><ul class='menu'></ul></div>"
      scale: 0.5
    include-in-header:
      text: "<link rel='stylesheet' href='https://cdn.jsdelivr.net/npm/bootstrap-icons@1.11.3/font/bootstrap-icons.min.css'><link rel='stylesheet' type='text/css' href='https://cdn.jsdelivr.net/gh/dreampulse/computer-modern-web-font@master/fonts.css'><link href='https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css' rel='stylesheet' integrity='sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH' crossorigin='anonymous'><script src='https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js' integrity='sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz' crossorigin='anonymous'></script>"
  html:
    output-file: index.html
    df-print: kable
    warning: false
    link-external-icon: true
    link-external-newwindow: true
---

::: {.content-visible unless-format="revealjs"}

<center>
<a class="h2" href="slides.html" target="_blank">Open slides in new window &rarr;</a>
</center>

:::

# Distributions in Continuous World {data-stack-name="Continuous World" .not-title-slide .title-12 .smaller .crunch-title .crunch-ul .crunch-img .crunch-math .math-80 .crunch-quarto-figure .highlight-col2}

::: {.hidden}

```{r}
#| label: r-source-globals
source("../dsan-globals/_globals.r")
```

{{< include ../_globals-tex.qmd >}}

:::

{{< include ../_components/discrete-vs-continuous.qmd >}}

## Moving to Continuous World {.crunch-title}

* Intuitions from discrete world *do* translate into good intuitions for continuous world, in this case!
* Can "move" discrete table into continuous space like Riemann sums "move" discrete sums into integrals:

![](images/riemann.svg){fig-align="center"}

## "Smoothing" Our Example {.text-95}

* Instead of discrete $G$ with $\mathcal{R}_G = \{10, 11, 12\}$, we have **continuous** $G$ with $\mathcal{R}_G = [10,12] \subset \mathbb{R}$ (HS *progress*)
* Instead of discrete $H$ with $\mathcal{R}_H = \{0, 1\}$, now we keep track of **continuous** "honors spectrum" $H$ with $\mathcal{R}_H = [0, 1] \subset \mathbb{R}$
* Student near beginning of 10th grade who is at "high end" of honors spectrum: $G = 10.03$ and $H = 0.95$

## "Smoothing" Our Tables

* **Sums** become **integrals**
* **Re-normalization** (ensuring that **probability mass** values **sum to 1**) becomes ensuring that **probability density** values **integrate to 1**.
* What comes in place of **frequency table**?
* Answer in theory: **Joint pdf**
* Answer in practice: Depends on the context ðŸ˜¬

## Continuous Joint pdfs {.smaller .crunch-title .crunch-ul .crunch-math}

* The **volume** of this **Hershey Kiss** is exactly $1$
* Integrating over a region $C$ gives us

$$
\frac{\text{Volume}(\{(X,Y) \mid (X,Y) \in C\})}{\text{Volume}(\text{Hershey Kiss})} = \Pr((X,Y) \in C)
$$

![Figure 3.11 from @degroot_probability_2013](images/fig-3-11.svg){fig-align="center"}

## Conditional pdfs {.smaller .crunch-title .crunch-li-8 .text-65}

* Now, if we **learn that $Y = y_0$**, can take "slice" at $y = y_0$
* **Total area** of this slice is **not** $1$, so $f_{X,Y}(x, y_0)$ is **not** a valid pdf
* **Dividing by total area of slice** would generate valid pdf. What is this area? $f_X(y_0)$
* $\implies$ $f_{X \mid Y = y_0}(x \mid y_0) = \frac{f_{X,Y}(x, y_0)}{f_X(y_0)}$ **is** valid (conditional) pdf

![Figure 3.20 from @degroot_probability_2013](images/fig-3-20.svg){fig-align="center"}

## Working Backwards in Continuous World {.title-08 .inline-95}

* While in **discrete world** we could easily provide a table, in **continuous world** we often/usually have to work backwards; we may just be given:
* $G \sim \mathcal{U}(10, 12)$
* $H \sim \ddot{\mathcal{N}}(\mu = 0.5, \sigma = 0.1, a = 0, b = 1)$, and
* $G \perp H$ (so $\Pr(G \mid H) = \Pr(G), \Pr(H \mid G) = \Pr(H)$)
* (i.e., marginal distributions = conditional distributions)

## The Marginal pdfs of $G$ and $H$ {.crunch-title .crunch-ul .text-95 .crunch-li-8}

* Since we know $G \sim \mathcal{U}(10,12)$, we know (or we could look up) that $G$ has pdf

$$
f_G(g) = \frac{1}{12 - 10} = \frac{1}{2}
$$

* $H$ has a slightly fancier distribution: **truncated normal**
* Nonetheless a pdf we can derive from (a) knowing the pdf of the **normal** distribution and (b) knowing what we've just talked about regarding **conditional** distributions

## The Truncated Normal Distribution {.smaller .crunch-title .crunch-ul}

* $\ddot{\mathcal{N}}$ may look scary, but $X \sim \ddot{\mathcal{N}}(\mu, \sigma, a, b)$ just means $X$ can be constructed as:

$$
X \sim \mathcal{N}(\mu, \sigma) \leadsto [X \mid a < X < b] \sim \ddot{\mathcal{N}}(\mu, \sigma, a, b)
$$

```{r}
#| label: truncated-plots
#| fig-cap: Adapted from [`simstudy` package documentation](https://www.rdatagen.net/post/generating-data-from-a-truncated-distribution/){target='_blank'}
rnormt <- function(n, range, mu, s = 1) {
  
  # range is a vector of two values
  
  F.a <- pnorm(min(range), mean = mu, sd = s)
  F.b <- pnorm(max(range), mean = mu, sd = s)
  
  u <- runif(n, min = F.a, max = F.b)
  
  qnorm(u, mean = mu, sd = s)
  
}
library(data.table)
library(simstudy)
library(paletteer)
defC <- defCondition(condition= "tt == 1", 
                     formula = "rnormt(10000, c(-Inf, Inf), mu = 0, s = 3)")
defC <- defCondition(defC, "tt == 2", 
                     formula = "rnormt(10000, c(0, Inf), mu = 0, s = 3)")
defC <- defCondition(defC, "tt == 3", 
                     formula = "rnormt(10000, c(-3, 3.5), mu = 0, s = 3)")

dd <- genData(30000)
dd <- trtAssign(dd, nTrt = 3, grpName = "tt")
dd <- addCondition(defC, dd, "x")
dd[, tt := factor(tt, 
     labels = c("No truncation", "Left truncation at 0", "Left and right truncation"))]
dd |> ggplot(aes(x = x, group = tt)) +
  geom_histogram(
    aes(fill = tt, y=after_stat(density)),
    alpha = 1, binwidth = .2, boundary = 0
  ) +
  facet_grid(~tt) +
  theme(
    panel.grid = element_blank(),
    axis.title = element_blank(),
  ) +
  dsan_theme() +
  remove_legend()
```

## The Truncated Normal pdf {.crunch-title .crunch-math .crunch-ul .math-90 .footnotes-0 .text-90 .inline-95}

* Since we see a **conditioning bar** on the previous slide, we can infer what the **pdf** of this **conditional distribution** would look like. If $X \sim \ddot{\mathcal{N}}(\mu, \sigma, a, b)$, then $X$ has pdf

$$
f_X(x) = \frac{1}{\sigma}\cdot \frac{\varphi\left(\frac{x - \mu}{\sigma}\right)}{\Phi\left(\frac{b-\mu}{\sigma}\right) - \Phi\left(\frac{a - \mu}{\sigma}\right)} \approx \frac{\Pr(X \approx x, a < X < b)}{\Pr(a < X < b)}
$$

* $\varphi$ is the pdf of $\mathcal{N}(0,1)$
* $\Phi$ is the CDF of $\mathcal{N}(0,1)$

::: {.aside}

(Note the lowercasing to describe pdfs and capitalizing to describe CDFs, even in Greek!)

:::

## Back to Working Backwards {.crunch-ul}

* By *definition of independence*, we can obtain **joint pdf** $f_{G,H}(g, h)$ by **multiplying** marginal pdf $f_G(g)$ and marginal pdf $f_H(h)$:

$$
\begin{align*}
f_{G,H}(g, h) &= f_G(g) \cdot f_H(h) \\
&= \frac{1}{2} \cdot \left[ \frac{1}{0.1} \cdot \frac
{
  \varphi\left(\frac{h - 0.5}{0.1}\right)
}
{
    \Phi\left(\frac{12-0.5}{0.1}\right) - \Phi\left(\frac{10 - 0.5}{0.1}\right)
}
\right]
\end{align*}
$$

## Moving Forwards Again {.crunch-title .math-90 .crunch-ul}

* We've arrived at the case we had in discrete world, where we know the **joint distribution**!
* We can **integrate** wherever we took **sums** in the discrete case to obtain **marginal** pdfs:

$$
\begin{align*}
f_G(g) &= \int_{0}^{1}f_{G,H}(g,h)\mathrm{d}h = \frac{1}{2}, \\
f_H(h) &= \int_{10}^{12}f_{G,H}(g, h)\mathrm{d}g = \frac{1}{0.1} \cdot \frac
{
  \varphi\left(\frac{h - 0.5}{0.1}\right)
}
{
  \Phi\left(\frac{12 - 0.5}{0.1}\right) - \Phi\left(\frac{10 - 0.5}{0.1}\right)
}
\end{align*}
$$

## Conditional Distributions in Continuous World {.text-90 .title-09}

* We can compute **conditional** pdfs by **renormalizing**
* Denominator is no longer integral of the distribution over **all** possible values (hence just the number $1$), but a **ratio** of **joint distribution** to **marginal distribution** values like:

$$
f_{H \mid G}(h | g) = \frac{f_{G,H}(g, h)}{f_G(g)}.
$$

## You Don't Have To Stare Helplessly At Scary Math!

* Try to **link the continuous equations back to their simpler discrete forms**
* Work with the discrete forms to develop intuition, then
* Convert **sums** back into **integrals** once you're ready

## A Concrete Strategy {.crunch-title .crunch-ul .crunch-figures .crunch-img .inline-95 .crunch-quarto-figure .text-90}

* Start by **discretizing** ("binning") the possible values of a **continuous** RV to obtain a **discrete** RV:
  * Split $[10,12]$ into three equal-length bins, $[0,1]$ into two equal-length bins
  * **Simulate** $(G, H)$ pairs, sort into bins, plot joint / marginal / conditional distributions of **binned data**
* As you make bins thinner and thinner... ðŸ¤”

![](images/riemann.svg){fig-align="center"}

# Thinking Through Specific Multivariate Distributions {data-stack-name="N > 2"}

* From $2$ to $N$ variables!

## The Multinoulli Distribution {.smaller .crunch-title .crunch-ul .crunch-math .inline-95}

* May seem like a weird/contrived distribution, but is **perfect for building intuition**, as your first $N$-dimensional distribution ($N > 2$)
* $\mathbf{X}$ is a six-dimensional **Vector-Valued** RV, so that
    
    $$
    \mathbf{X} = (X_1, X_2, X_3, X_4, X_5, X_6),
    $$
    
    where $\mathcal{R}_{X_1} = \{0, 1\}, \mathcal{R}_{X_2} = \{0, 1\}, \ldots, \mathcal{R}_{X_6} = \{0, 1\}$
* But, $X_1, X_2, \ldots, X_6$ are **not independent!** In fact, they are **so dependent** that if one has value $1$, the rest must have value $0$ $\leadsto$ we can infer the support of $\mathbf{X}$:
    
    $$
    \begin{align*}
    \mathcal{R}_{\mathbf{X}} = \{ &(1,0,0,0,0,0),(0,1,0,0,0,0),(0,0,1,0,0,0), \\
    &(0,0,0,1,0,0),(0,0,0,0,1,0),(0,0,0,0,0,1)\}
    \end{align*}
    $$

* Lastly, need to define the **probability** that $\mathbf{X}$ takes on any of these values. Let's say $\Pr(\mathbf{X} = \mathbf{v}) = \frac{1}{6}$ for all $\mathbf{v} \in \mathcal{R}_{\mathbf{X}}$. Do we see the structure behind this contrived case?
* (For math major friends, there is an isomorphism afoot... For the rest, it's an extremely inefficient way to model outcomes from **rolling a fair die**)

## The Multivariate Normal Distribution {.smaller .crunch-title .title-10 .crunch-ul .crunch-ul .math-90 .crunch-math}

* We've already seen the matrix notation for writing the **parameters** of this distribution: $\mathbf{X}_{[k \times 1]} \sim \mathcal{N}_k\left(\boldsymbol\mu_{[k \times 1]}, \Sigma_{[k \times k]}\right)$
* Now we get to crack open the matrix notation for writing its **pdf**:

$$
f_\mathbf{X}(\mathbf{x}_{[k \times 1]}) = \underbrace{\left(\frac{1}{\sqrt{2\pi}}\right)^k \frac{1}{\sqrt{\det(\Sigma)}}}_{\text{Normalizing constants}} \exp\left(-\frac{1}{2}\underbrace{(\mathbf{x} - \boldsymbol\mu)^\top \Sigma^{-1} (\mathbf{x} - \boldsymbol\mu)}_{\text{Quadratic form}}\right)
$$

* Try to squint your eyes while looking at the above and compare with...

:::: {layout="[1,1]" layout-valign="top"}
::: {#mvnorm-compare-left}

<center>

[pdf of $\mathcal{N}(\mu,\sigma)$]{style="border-bottom: 1px solid black;"}

</center>

$$
f_X(v) = \frac{1}{\sigma\sqrt{2\pi}}\bigexp{-\frac{1}{2}\left(\frac{v - \mu}{\sigma}\right)^2}
$$

:::
::: {#mvnorm-compare-right}

<center>

[Structure of $\Sigma$]{style="border-bottom: 1px solid black;"}

</center>

$$
\begin{align*}
\mathbf{\Sigma} &= \begin{bmatrix}\sigma_1^2 & \rho\sigma_1\sigma_2 \\ \rho\sigma_2\sigma_1 & \sigma_2^2\end{bmatrix} \\[0.1em]
\implies \det(\Sigma) &= \sigma_1^2\sigma_2^2 - \rho^2\sigma_1^2\sigma_2^2 \\
&= \sigma_1^2\sigma_2^2(1-\rho^2)
\end{align*}
$$

:::
::::

## Quadratic Forms {.smaller}

* Quadratic forms will seem scary until someone forces you to write out the matrix multiplication!
* Start with the 1D case: $\mathbf{x} = [x_1]$, $\boldsymbol\mu = [\mu_1]$, $\Sigma = [\sigma^2]$. Then

$$
(\mathbf{x} - \boldsymbol\mu)^\top \Sigma^{-1} (\mathbf{x - \boldsymbol\mu}) = (x_1 - \mu_1)\frac{1}{\sigma^2}(x_1 - \mu_1) = \left(\frac{x_1 - \mu_1}{\sigma}\right)^2 ~ ðŸ¤¯
$$

## The 2D Case {.smaller .math-80 .crunch-title .crunch-ul .crunch-lists .crunch-math}

* Let $\mathbf{x} = \left[\begin{smallmatrix}x_1 \\ x_2\end{smallmatrix}\right]$, $\boldsymbol\mu = \left[ \begin{smallmatrix}\mu_1 \\ \mu_2 \end{smallmatrix}\right]$, $\Sigma$ as in previous slide. Then $\mathbf{x} - \boldsymbol\mu = \left[ \begin{smallmatrix} x_1 - \mu_1 \\ x_2 - \mu_2 \end{smallmatrix} \right]$
* Using what we know about $2 \times 2$ matrix inversion,

$$
\Sigma^{-1} = \frac{1}{\det(\Sigma)}\left[ \begin{smallmatrix} \sigma_2^2 & -\rho \sigma_2\sigma_1 \\ -\rho \sigma_1\sigma_2 & \sigma_1^2\end{smallmatrix} \right] = \frac{1}{\sigma_1^2\sigma_2^2(1-\rho^2)}\left[ \begin{smallmatrix} \sigma_2^2 & -\rho \sigma_2\sigma_1 \\ -\rho \sigma_1\sigma_2 & \sigma_1^2\end{smallmatrix} \right]
$$

* With $\widetilde{\rho} \equiv 1 - \rho^2$, we can write everything as a bunch of matrix multiplications:

$$
\begin{align*}
&(\mathbf{x} - \boldsymbol\mu)^\top \Sigma^{-1} (\mathbf{x} - \boldsymbol\mu)
= \frac{1}{\sigma_1^2\sigma_2^2 \widetilde{\rho}}
\begin{bmatrix}x_1 - \mu_1 & x_2 - \mu_2\end{bmatrix}
\cdot
\begin{bmatrix} \sigma_2^2 & -\rho \sigma_2\sigma_1 \\ -\rho \sigma_1\sigma_2 & \sigma_1^2\end{bmatrix}
\cdot
\begin{bmatrix}x_1 - \mu_1 \\ x_2 - \mu_2\end{bmatrix} \\
&= \frac{1}{\sigma_1^2\sigma_2^2 \widetilde{\rho}}
\begin{bmatrix}(x_1-\mu_1)\sigma_2^2 - (x_2-\mu_2)\rho\sigma_1\sigma_2 & (x_2 - \mu_2)\sigma_1^2 - (x_1 - \mu_1)\rho\sigma_2\sigma_1 \end{bmatrix}
\cdot
\begin{bmatrix}x_1 - \mu_1 \\ x_2 - \mu_2\end{bmatrix} \\
&= \frac{1}{\sigma_1^2\sigma_2^2 \widetilde{\rho}}
\left(
  (x_1-\mu_1)^2 \sigma_2^2 - (x_1 - \mu_1)(x_1 - \mu_2)\sigma_1\sigma_2 + (x_2 - \mu_2)^2 \sigma_1^2 - (x_1 - \mu_1)(x_2 - \mu_2)\sigma_2\sigma_1
\right) \\
&= \boxed{
\frac{1}{\widetilde{\rho}}
\left(
  \left(\frac{x_1 - \mu_1}{\sigma_1}\right)^2
  + \left(\frac{x_2 - \mu_2}{\sigma_2}\right)^2
  - 2\rho\frac{(x_1 - \mu_1)(x_2 - \mu_2)}{\sigma_1\sigma_2}
\right)
}
\end{align*}
$$

## The 2D Case In Its FINAL FORM {.smaller .math-90}

$$
f_{\mathbf{X}}(\mathbf{x})
= C \bigexp{
  -\frac{1}{2(1-\rho^2)}
  \left(
    \left(\frac{x_1 - \mu_1}{\sigma_1}\right)^2
    + \left(\frac{x_2 - \mu_2}{\sigma_2}\right)^2
    - 2\rho\frac{(x_1 - \mu_1)(x_2 - \mu_2)}{\sigma_1\sigma_2}
  \right)
}
$$

where

$$
C = \frac{1}{2\pi\sigma_1\sigma_2\sqrt{1-\rho^2}}.
$$

## References

::: {#refs}
:::
